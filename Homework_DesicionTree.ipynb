{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем необходимые библиотеки"
      ],
      "metadata": {
        "id": "7Eu7Thk4KD2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XBPcQuUYJ6cZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_iris\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris_dataset = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
        "df['target'] = iris_dataset.target\n",
        "df['target'] = df.target.apply(lambda v: iris_dataset.target_names[v])\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "2n4Vhg7DKRro",
        "outputId": "a4f6c4d7-51da-4790-8156-316c8ed9db38"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target  \n",
              "0  setosa  \n",
              "1  setosa  \n",
              "2  setosa  \n",
              "3  setosa  \n",
              "4  setosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3caf6c7c-4774-4cf9-bf15-204c2e427ca3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3caf6c7c-4774-4cf9-bf15-204c2e427ca3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3caf6c7c-4774-4cf9-bf15-204c2e427ca3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3caf6c7c-4774-4cf9-bf15-204c2e427ca3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим функцию, вычисляющую энтропию"
      ],
      "metadata": {
        "id": "nq3PdcnwKZkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from math import log2\n",
        "\n",
        "def entropy(targets: pd.Series) -> float:\n",
        "    class_probas = targets.value_counts(normalize=True)\n",
        "    return -class_probas.apply(lambda p: p * log2(p)).sum()\n",
        "eps = 1e-7\n"
      ],
      "metadata": {
        "id": "hN3tz17UKUSA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка условий на истинность"
      ],
      "metadata": {
        "id": "VsHY8zmXKlJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert abs(entropy(pd.Series(['a', 'a', 'a'])) - 0.) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'a', 'b'])) - 0.9182958340) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'a', 'b', 'b', 'c', 'a', 'a', 'b'])) - 1.4056390622) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'b', 'c', 'd'])) - 2) < eps\n"
      ],
      "metadata": {
        "id": "a5e1D8KjKn1U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию, которая применяется для оценки качества разбиения при решении задач классификации. Функция будет вычислять значение энтропии после разделения выборкина две части"
      ],
      "metadata": {
        "id": "vg0KUzy3K1n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(before_split: pd.Series, split_left: pd.Series, split_right: pd.Series) -> float:\n",
        "    e_before_split = entropy(before_split)\n",
        "    e_left, left_proportion = entropy(split_left), len(split_left) / len(before_split)\n",
        "    e_right, right_proportion = entropy(split_right), len(split_right) / len(before_split)\n",
        "    return (e_before_split - left_proportion * e_left - right_proportion * e_right)\n"
      ],
      "metadata": {
        "id": "_2dGoZ9YMocP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert abs(information_gain(pd.Series(['a', 'a', 'b', 'b']), pd.Series(['a', 'a']), pd.Series(['b', 'b'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'b', 'c', 'b']), pd.Series(['a', 'c']), pd.Series(['b', 'b'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'b', 'c', 'd']), pd.Series(['a', 'b']), pd.Series(['c', 'd'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'a', 'c', 'd']), pd.Series(['a', 'c']), pd.Series(['a', 'd'])) - 0.5) < eps\n"
      ],
      "metadata": {
        "id": "TwTMGgYWMwRJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь напишем функцию, которая разбивает переданные данные на части по указанному количеству шагов, обучает модель на каждой части и возвращает среднюю метрику качества модели и название используемого классификатора"
      ],
      "metadata": {
        "id": "xL_U-ECsMwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def split_by_feature(x: pd.DataFrame, y: pd.Series, feature: str, steps: int = 100) -> Tuple[float, float]:\n",
        "    min_value, max_value = x[feature].min(), x[feature].max()\n",
        "    thresholds = np.linspace(min_value, max_value, steps)\n",
        "    best_gain, best_threshold = 0, None\n",
        "    x_parts = np.array_split(x[feature], steps)\n",
        "    y_parts = np.array_split(y, steps)\n",
        "    for threshold in thresholds:\n",
        "        ### Ваш код\n",
        "        info_gains = []\n",
        "        for i in range(steps):\n",
        "            y_true = y_parts[i]\n",
        "            split_left = y_true[x_parts[i] <= threshold]\n",
        "            split_right = y_true[x_parts[i] > threshold]\n",
        "            info_gains.append(information_gain(y_true, split_left, split_right))\n",
        "        mean_info_gain = sum(info_gains) / len(info_gains)\n",
        "        if mean_info_gain > best_gain:\n",
        "            bast_gain = mean_info_gain\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_threshold\n"
      ],
      "metadata": {
        "id": "FZWo896qNAsD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В описсанной ниже функции разбиения возвращается наилучший информационный выигрыш, соответствующий столбцу, индексу столбца и пороговому значению"
      ],
      "metadata": {
        "id": "R02DtMELNIlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_by_features(x: pd.DataFrame, y: pd.Series, steps: int = 100) -> Tuple[float, str]:\n",
        "    best_gain, best_threshold, best_feature, best_idx = 0, None, None, None\n",
        "\n",
        "    for idx, feature in enumerate(x.columns):\n",
        "        ### Ваш код\n",
        "        thresholds = np.linspace(x[feature].min(), x[feature].max(), steps)\n",
        "        for threshold in thresholds:\n",
        "            split_left = y[x[feature] <= threshold]\n",
        "            split_right = y[x[feature] > threshold]\n",
        "            gain = information_gain(y, split_left, split_right)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature\n",
        "                best_idx = idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_feature, best_idx, best_threshold"
      ],
      "metadata": {
        "id": "np_jJZ25NKhn"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опишем вершину принятия решений. В вершине мы либо храним правило, либо доминантный класс."
      ],
      "metadata": {
        "id": "7aRej8MCNh4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PMn2TqmCMqR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Optional\n",
        "\n",
        "@dataclass\n",
        "class DecisionNode:\n",
        "    feature: Optional[str] = None\n",
        "    feature_idx: Optional[int] = None\n",
        "    threshold: Optional[float] = None\n",
        "    dominative_class: Any = None\n",
        "\n",
        "    @classmethod\n",
        "    def make_leaf(cls, values: pd.Series) -> 'DecisionNode':\n",
        "        class_counts = values.value_counts().to_dict()\n",
        "        dominative_class = max(class_counts, key=lambda c: class_counts[c])\n",
        "        return cls(dominative_class=dominative_class)\n",
        "\n",
        "    @classmethod\n",
        "    def make_node(\n",
        "        cls, best_feature: str, best_feature_idx: int, best_threshold: float\n",
        "    ) -> 'DecisionNode':\n",
        "        return cls(\n",
        "            feature=best_feature,\n",
        "            feature_idx=best_feature_idx,\n",
        "            threshold=best_threshold\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self) -> bool:\n",
        "        return bool(self.dominative_class)\n",
        "\n"
      ],
      "metadata": {
        "id": "DWHbaX1rNgVn"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим класс. Метод fit будет отвечать за тренировку классификатора. Он перебирает все признаки и все уникальные значения каждого признака, строит решающее правило на основе признака и порога, вычисляет ошибку классификации для каждого правила и выбирает правило с минимальной ошибкой.\n",
        "\n",
        "\n",
        "Метод predict использует сохраненный ранее узел дерева, который содержит информацию о признаке, пороге, левом и правом поддеревьях, исходных данных и метках листьев. Затем он рекурсивно проходит по дереву, начиная с корневого узла, и классифицирует новые данные, используя правила, сохраненные в каждом узле."
      ],
      "metadata": {
        "id": "56Lu9rjSSVP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        # Будем хранить внутри пня информацию о лучшем признаке и лучшей границе этого признака\n",
        "        self._node: Optional[DecisionNode] = None\n",
        "        self._left_node: Optional[DecisionNode] = None\n",
        "        self._right_node: Optional[DecisionNode] = None\n",
        "        self.feature_index = None\n",
        "    \n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
        "        # _, feature, feature_idx, threshold = split_by_features(X_train, y_train)\n",
        "        # x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n",
        "        # y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n",
        "        # ### Ваш код\n",
        "        # self.feature = None  # инициализируем признак, на котором делается разбиение\n",
        "        # self.threshold = None  # инициализируем порог разбиения\n",
        "        # self.alpha = None  # инициализируем вес классификатора\n",
        "        # best_error = float(\"inf\")  # инициализируем минимальную ошибку классификации\n",
        "\n",
        "        # for feature in X_train.columns:  # перебираем все признаки\n",
        "        #     for threshold in X_train[feature].unique():  # перебираем все уникальные значения признака\n",
        "        #         # строим правило на основе признака и порога\n",
        "        #         y_pred = np.where(X_train[feature] <= threshold, -1, 1)\n",
        "        #         # вычисляем ошибку классификации\n",
        "        #         error = np.sum(y_pred != y_train)\n",
        "        #         if error < best_error:  # если ошибка меньше текущей минимальной\n",
        "        #             best_error = error  # обновляем минимальную ошибку\n",
        "        #             self.feature = feature  # сохраняем признак и порог\n",
        "        #             self.threshold = threshold\n",
        "\n",
        "        # # вычисляем вес классификатора\n",
        "\n",
        "        # self.alpha = 0.5 * np.log((1 - best_error) / best_error)\n",
        "        _, feature, feature_idx, threshold = split_by_features(X_train, y_train)\n",
        "        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n",
        "        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n",
        "        self._node = DecisionNode.make_node(feature, feature_idx, threshold)\n",
        "        self._left_node = DecisionNode.make_leaf(y_left)\n",
        "        self._right_node = DecisionNode.make_leaf(y_right)\n",
        "\n",
        "        \n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
        "        # x = X.copy()\n",
        "        # node = self._node\n",
        "        # left_samples = x[x[node.feature] < node.threshold]\n",
        "        # right_samples = x[x[node.feature] >= node.threshold]\n",
        "        # ### Ваш код\n",
        "        # predictions = pd.Series(np.zeros(len(x)), index=x.index)\n",
        "\n",
        "        # while node:\n",
        "        #     if node.is_leaf:\n",
        "        #         predictions.loc[x.index.intersection(node.samples.index)] = node.label\n",
        "        #         break\n",
        "        #     left_mask = left_samples.index.isin(x.index)\n",
        "        #     right_mask = right_samples.index.isin(x.index)\n",
        "        #     predictions.loc[left_mask] = node.left.predict(left_samples)\n",
        "        #     predictions.loc[right_mask] = node.right.predict(right_samples)\n",
        "        #     node = node.left if x.loc[left_mask, node.feature].all() else node.right\n",
        "        \n",
        "        # return predictions\n",
        "\n",
        "        # создаем серию с предсказаниями и возвращаем ее\n",
        "        # predictions = pd.Series(index=X.index)\n",
        "        # for i, row in X.iterrows():\n",
        "        #     predictions[i] = self._predict_row(row)\n",
        "        # return predictions\n",
        "\n",
        "        x = X.copy()\n",
        "        node = self._node\n",
        "        left_samples = x[x[node.feature] < node.threshold]\n",
        "        right_samples = x[x[node.feature] >= node.threshold]\n",
        "        x.loc[left_samples.index, \"pred\"] = self._left_node.dominative_class\n",
        "        x.loc[right_samples.index,\"pred\"] = self._right_node.dominative_class\n",
        "        return x.pred\n",
        "\n",
        "    def _predict_row(self, row: pd.Series) -> int:\n",
        "        # делаем предсказание для одной строки\n",
        "        feature_value = row[self._node.feature_index]\n",
        "        if feature_value < self._node.threshold:\n",
        "            return self._left_node.prediction\n",
        "        else:\n",
        "            return self._right_node.prediction\n",
        "\n",
        "    def fit_predict(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
        "        self.fit(X, y)\n",
        "        return self.predict(X)\n"
      ],
      "metadata": {
        "id": "KW8uEAdNSVlm"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим "
      ],
      "metadata": {
        "id": "_ZAHlC0NN5BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "stump = DecisionStump()\n",
        "stump.fit(X_train, y_train)\n",
        "y_pred = stump.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "stump._node.feature, stump._node.threshold\n",
        "\n",
        "\n",
        "reduced_df = df[df.target.apply(lambda t: t in ['setosa', 'versicolor'])]\n",
        "X, y = reduced_df.drop(columns=['target']), reduced_df.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.35, random_state=42)\n",
        "\n",
        "print(len(X_train), len(X_test))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4coEJTFYJYRk",
        "outputId": "378036e4-b3e5-42b6-c3a4-5e257a1c7289"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      0.93      0.96        29\n",
            "  versicolor       0.48      1.00      0.65        23\n",
            "   virginica       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.67        75\n",
            "   macro avg       0.49      0.64      0.54        75\n",
            "weighted avg       0.53      0.67      0.57        75\n",
            "\n",
            "35 65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "@dataclass\n",
        "class DecisionNode:\n",
        "    # Решающие признаки остаются как есть\n",
        "    feature: Optional[str] = None\n",
        "    feature_idx: Optional[int] = None\n",
        "    threshold: Optional[float] = None\n",
        "    # Добавляются ссылки на левую и правую вершины\n",
        "    left: Optional['DecisionNode'] = None\n",
        "    right: Optional['DecisionNode'] = None\n",
        "    # Класс в листе остается как есть\n",
        "    dominative_class: Any = None\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self) -> bool:\n",
        "        return bool(self.dominative_class)\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_impurity_spllit: float = 0.01, min_leaf_split=2, max_depth=None):\n",
        "        self._root: Optional[DecisionNode] = None\n",
        "        # В качестве простых эвристики для остановки возьмем:\n",
        "        # - максимальную глубину ветви\n",
        "        # - минимально допустимое число примеров в узле\n",
        "        # - долю \"примесей\" в узле\n",
        "        self._max_impurity_spllit = max_impurity_spllit\n",
        "        self._min_leaf_split = min_leaf_split\n",
        "        self._max_depth = max_depth\n",
        "\n",
        "    def _fit(self, X_train: pd.DataFrame, y_train: pd.Series, depth=0):\n",
        "        # проверим, можно ли вернуть лист\n",
        "        classes_probas = y_train.value_counts(normalize=True).to_dict()\n",
        "        dominative_class = max(classes_probas, key=lambda c: classes_probas[c])\n",
        "        impurity = 1 - classes_probas[dominative_class]\n",
        "        leaf_size = len(X_train)\n",
        "        if impurity <= self._max_impurity_spllit or leaf_size < self._min_leaf_split or (self._max_depth and depth >= self._max_depth):\n",
        "            return DecisionNode(dominative_class=dominative_class)\n",
        "        # если нет, продолжаем идти рекурсивно\n",
        "        # делаем перебор максимум в 10 точках\n",
        "        _, feature, feature_idx, threshold = split_by_features(X_train, y_train, 10)\n",
        "        if not feature:  # если не получается выбрать оптимальный признак для разбиения - завершаем перебор\n",
        "            return DecisionNode(dominative_class=dominative_class)\n",
        "        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n",
        "        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n",
        "        left_node = self._fit(x_left, y_left, depth + 1)\n",
        "        right_node = self._fit(x_right, y_right, depth + 1)\n",
        "        return DecisionNode(\n",
        "            feature=feature,\n",
        "            feature_idx=feature_idx,\n",
        "            threshold=threshold,\n",
        "            left=left_node,\n",
        "            right=right_node,\n",
        "        )\n",
        "\n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        self._root = self._fit(X_train, y_train)\n",
        "\n",
        "    def _predict(self, sample: pd.Series, node: DecisionNode) -> Any:\n",
        "        if node.is_leaf:\n",
        "            return node.dominative_class\n",
        "        if sample[node.feature] < node.threshold:\n",
        "            return self._predict(sample, node.left)\n",
        "        return self._predict(sample, node.right)\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
        "        return X.apply(lambda sample: self._predict(sample, self._root), axis=1)\n",
        "\n",
        "    def fit_predict(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        self.fit(X_train, y_train)\n",
        "        return self.predict(X_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X, y = df.drop(columns=['target']), df.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)\n",
        "\n",
        "print(len(X_train), len(X_test))\n",
        "tree = DecisionTree()\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ-0FKdaZXIh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "a7d59f0c-4e80-40b6-a707-351de9ab1a54"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75 75\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_train, y_train)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDecisionNode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, depth)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mright_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         return DecisionNode(\n\u001b[1;32m     47\u001b[0m             \u001b[0mfeature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mright_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         return DecisionNode(\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mright_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         return DecisionNode(\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, depth)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mleft_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mright_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         return DecisionNode(\n",
            "\u001b[0;32m<ipython-input-36-53f5fb5eac37>\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X_train, y_train, depth)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# проверим, можно ли вернуть лист\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mclasses_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mdominative_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses_probas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mclasses_probas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mimpurity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclasses_probas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdominative_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mleaf_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4SuDEbgdAqOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вторая часть ДЗ\n",
        "\n",
        "В этой домашней работе вам предстоит научиться предсказывать цены товаров из маркетплейса Azamon.\n",
        "\n",
        "Требования к домашней работе:\n",
        "- Во всех графиках должны быть подписи через title, legend, etc.\n",
        "- Во время обучения моделей проверяйте, что у вас не текут данные. Обычно это позитивно влияет на качество модели на тесте, но негативно влияет на оценку 🌚\n",
        "- Если вы сдаете работу в Google Colaboratory, убедитесь, что ваша тетрадка доступна по ссылке.\n",
        "- Использование мемов допускается, но необходимо соблюдать меру. Несодержательная работа, состоящая только из мемов, получает 0 баллов."
      ],
      "metadata": {
        "id": "GKtOnum5ajUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашняя работа: деревья решений"
      ],
      "metadata": {
        "id": "JUCwbsTeApZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт модулей"
      ],
      "metadata": {
        "id": "fo5AM45m8rPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n"
      ],
      "metadata": {
        "id": "Eu-Ss-gG3qEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('amazon_co-ecommerce_sample.csv').drop(columns=[\n",
        "    'product_name',\n",
        "    'index',\n",
        "    'uniq_id',\n",
        "    'customers_who_bought_this_item_also_bought',\n",
        "    'items_customers_buy_after_viewing_this_item',\n",
        "    'sellers',\n",
        "    'description', # text\n",
        "    'product_information', # text\n",
        "    'product_description', # text\n",
        "    'customer_questions_and_answers', # text\n",
        "    'customer_reviews', # text\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "fPAWhrX78stE",
        "outputId": "ff066cf5-4f2b-4465-f107-9dd675eea494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-61b7572c3f92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = pd.read_csv('amazon_co-ecommerce_sample.csv').drop(columns=[\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'product_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'uniq_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'customers_who_bought_this_item_also_bought'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 18 fields in line 318, saw 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Очистка данных (1 балл)\n",
        "\n",
        "Посмотрите на признаки. Есть ли в них пропуски? Какое соотношение между NaN'ами и общим количеством данных? Есть ли смысл выкидывать какие-либо данные из этого датасета?"
      ],
      "metadata": {
        "id": "quHYfDWdAx5p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считывание таблицы и удаление некоторых колонок"
      ],
      "metadata": {
        "id": "7XaKMeiN8vQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getna(df):\n",
        "    print(\"Количество nan | имя топика | процент потери от общего числа\")\n",
        "    for column in df.columns:\n",
        "        #print(df[column])\n",
        "        print(df[column].isnull().sum(), \"|\", column, \"|\", df[column].isnull().sum() / len(df[column]) * 100, \"%\")\n",
        "\n",
        "getna(df)"
      ],
      "metadata": {
        "id": "6gIUCsGQ8x1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод кол-ва NaN-ов, имени топика и соотношения между общим кол-вом данных и NaN-ов"
      ],
      "metadata": {
        "id": "WpJu5-gk8zQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выкидывать данные из датасета не стоит, потому что все эти фичи крайне вероятно, что\n",
        " напрямую или косвенно будут влиять на будущую цену товара\n",
        "\n",
        " Заполнение пропусков в числовых признаках:\n"
      ],
      "metadata": {
        "id": "u3uAPC0k9OIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подготовка данных (3 балла)\n",
        "\n",
        "Обработайте признаки. Выполните кодирование категориальных признаков, заполните пропуски в числовых признаках. Обратите внимание, что в датасете есть признак, который разбивается на несколько подпризнаков. Что это за признак? Закодируйте и его.\n",
        "\n",
        "Дополнительные вопросы (+ 1 балл):\n",
        "- Какие из признаков в этом датасете лучше кодировать через ordinal encoding?\n",
        "- Какие из признаков допустимо кодировать через one-hot?\n",
        "\n",
        "Прим.: суммарно за эту секцию можно получить до 4 баллов."
      ],
      "metadata": {
        "id": "AHAtw5rZA3Q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка со средним пользовательским рейтингом переводится в систему оценки от 0 до 1"
      ],
      "metadata": {
        "id": "HNkKosXY9kGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formean = sum([float(x.split()[0]) / float(x.split()[-2]) for x in df[\"average_review_rating\"] if type(x) == str])\n",
        "print(formean / len(df)) # Mean value\n",
        "df[\"average_review_rating\"] = df[\"average_review_rating\"].fillna(f\"{formean * 5} out of 5 stars\")\n",
        "df[\"average_review_rating\"] = [float(x.split()[0]) / float(x.split()[-2]) for x in df[\"average_review_rating\"]]\n",
        "print(df[\"average_review_rating\"])"
      ],
      "metadata": {
        "id": "4kfMRXDB9LE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с кол-вом ответов на вопросы заполняется, где недостача, средними значениями"
      ],
      "metadata": {
        "id": "QaCh5DRA9i7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_answered_questions\"] = df[\"number_of_answered_questions\"].astype(float)\n",
        "df[\"number_of_answered_questions\"] = df[\"number_of_answered_questions\"].fillna(df[\"number_of_answered_questions\"].mean())\n",
        "print(df[\"number_available_in_stock\"])\n"
      ],
      "metadata": {
        "id": "_SSsiM3A9hHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с оценками заполняется "
      ],
      "metadata": {
        "id": "HhhU8To395IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].fillna(df[\"number_of_answered_questions\"]).astype(str)\n",
        "df[\"number_of_reviews\"] = [x.replace(\",\", \".\") for x in df[\"number_of_reviews\"]]\n",
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].astype(float)\n",
        "print(df[\"number_of_reviews\"])"
      ],
      "metadata": {
        "id": "EtL72d4p96Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка кол-ва ревью преобразовывается в float тип и заполняется средними значениями"
      ],
      "metadata": {
        "id": "98occ7wcA-tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].apply(lambda x: float(str(x).replace(\",\", '.')))\n",
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].fillna(df[\"number_of_reviews\"].mean())\n",
        "print(df[\"number_of_reviews\"])"
      ],
      "metadata": {
        "id": "4UGNO5NOBJNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка цен преобразовывается к float и заполняется средними значениями"
      ],
      "metadata": {
        "id": "O8-r8z-CBLxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.price = df.price.apply(lambda x: float(str(x).replace(\"£\", \"\").replace(\",\", \"\").split()[0]))\n",
        "df.price = df.price.fillna(df.price.mean())\n",
        "print(df.price)"
      ],
      "metadata": {
        "id": "o6jHb4oxDO_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка количества товара в наличии заполняется нулями, если товар отсутствует"
      ],
      "metadata": {
        "id": "TrSz8q6pMyKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_available_in_stock\"] = df[\"number_available_in_stock\"].apply(lambda x: float(str(x).split()[0]))\n",
        "df[\"number_available_in_stock\"] = df[\"number_available_in_stock\"].fillna(0)\n",
        "print(df[\"number_available_in_stock\"])"
      ],
      "metadata": {
        "id": "xiHHkyqzM3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Категориальные признаки"
      ],
      "metadata": {
        "id": "wzD8LxiKMDRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка производителей с помощью Ordinal encoding кодируется"
      ],
      "metadata": {
        "id": "FA_WhPWQDRTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"manufacturer\"].fillna(df[\"manufacturer\"].value_counts().index[0])\n",
        "ll = LabelEncoder()\n",
        "X = ll.fit_transform(X)\n",
        "df[\"manufacturer\"] = X\n",
        "print(df[\"manufacturer\"])"
      ],
      "metadata": {
        "id": "KZFNF-grNGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с категориями амазон - колонка, которая представляет собой еще один датафрейм. "
      ],
      "metadata": {
        "id": "iDXnCWbeNIAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ll = LabelEncoder()\n",
        "df[\"amazon_category_and_sub_category\"] = df[\"amazon_category_and_sub_category\"].fillna(df[\"amazon_category_and_sub_category\"].mode().iloc[0])\n",
        "df[\"amazon_category_and_sub_category\"] = ll.fit_transform(df[\"amazon_category_and_sub_category\"].values)\n",
        "print(df[\"amazon_category_and_sub_category\"])"
      ],
      "metadata": {
        "id": "YELwckL2NRLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yamLHMjTtaRj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Обучите базовую модель. Для этого используйте `sklearn.dummy.DummyRegressor`. Какое качество она показывает на тесте? Посчитайте MSE, RMSE."
      ],
      "metadata": {
        "id": "tCe_dKUI_54i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "DRegr = DummyRegressor()\n",
        "X = df.drop(columns=\"price\")\n",
        "y = df.price"
      ],
      "metadata": {
        "id": "hIxwPrnD_9Hv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
      ],
      "metadata": {
        "id": "svoTlQrjAAgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Разделяем данные на обучающую выборку и тестовую"
      ],
      "metadata": {
        "id": "C6SvzZqCADFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DRegr.fit(X_train, y_train)\n",
        "y_pred = DRegr.predict(X_test)\n",
        "print(DRegr.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "M51xN6JHAHeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Модель стала чуть хуже: Коэффициент детерминации прогноза, идеально 1, если меньше 0, то стало похуже\n"
      ],
      "metadata": {
        "id": "F3Q9J_CVANSq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "hUgmf2teAYWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MSE = mean_squared_error(y_test, y_pred, squared=True)\n",
        "RMSE = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(MSE, RMSE)"
      ],
      "metadata": {
        "id": "sTXbK6RzAVpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление значения функции ошибки с помощью встроенной в sklearn функции,\n",
        "squared = True возвращает MSE, если False - RMSE"
      ],
      "metadata": {
        "id": "NFOJaTW5AWNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Дерево решений\n",
        "\n",
        "Обучите регрессионное дерево решений, проверьте качество этой модели на тестовой выборке. Улучшилось ли качество по сравнению с базовой моделью? Оцените r2_score обученной модели."
      ],
      "metadata": {
        "id": "k_bVs3OcAc6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "DecTree = DecisionTreeRegressor()\n",
        "DecTree.fit(X_train, y_train)\n",
        "\n",
        "y_pred_dec = DecTree.predict(X_test)\n",
        "\n",
        "print(r2_score(y_test, y_pred_dec))"
      ],
      "metadata": {
        "id": "YU_tik_iAdsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Линейная регрессия\n",
        "\n",
        "Попробуйте обучить линейную регрессию с параметрами по умолчанию. Оцените r2_score на тестовой выборке. Сравните качество с деревом решений. "
      ],
      "metadata": {
        "id": "nTKFrkmjAg6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "LinReg = LinearRegression()\n",
        "LinReg.fit(X_train, y_train)\n",
        "\n",
        "y_pred_lr = LinReg.predict(X_test)\n",
        "print(r2_score(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "gnn3Tq0RAj9k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}