{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Подключаем необходимые библиотеки"
      ],
      "metadata": {
        "id": "7Eu7Thk4KD2t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBPcQuUYJ6cZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_iris\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris_dataset = load_iris()\n",
        "\n",
        "df = pd.DataFrame(data=iris_dataset.data, columns=iris_dataset.feature_names)\n",
        "df['target'] = iris_dataset.target\n",
        "df['target'] = df.target.apply(lambda v: iris_dataset.target_names[v])\n",
        "\n",
        "print(len(df))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "2n4Vhg7DKRro",
        "outputId": "71ea5b28-37e9-444b-fe7a-aec2b65f9a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "   target  \n",
              "0  setosa  \n",
              "1  setosa  \n",
              "2  setosa  \n",
              "3  setosa  \n",
              "4  setosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e331216-600e-4558-a581-05643b069329\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e331216-600e-4558-a581-05643b069329')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-9e331216-600e-4558-a581-05643b069329 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-9e331216-600e-4558-a581-05643b069329');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим функцию, вычисляющую энтропию"
      ],
      "metadata": {
        "id": "nq3PdcnwKZkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from math import log2\n",
        "\n",
        "def entropy(targets: pd.Series) -> float:\n",
        "    class_probas = targets.value_counts(normalize=True)\n",
        "    return -class_probas.apply(lambda p: p * log2(p)).sum()\n",
        "eps = 1e-7\n"
      ],
      "metadata": {
        "id": "hN3tz17UKUSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка условий на истинность"
      ],
      "metadata": {
        "id": "VsHY8zmXKlJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assert abs(entropy(pd.Series(['a', 'a', 'a'])) - 0.) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'a', 'b'])) - 0.9182958340) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'a', 'b', 'b', 'c', 'a', 'a', 'b'])) - 1.4056390622) < eps\n",
        "assert abs(entropy(pd.Series(['a', 'b', 'c', 'd'])) - 2) < eps\n"
      ],
      "metadata": {
        "id": "a5e1D8KjKn1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию, которая применяется для оценки качества разбиения при решении задач классификации. Функция будет вычислять значение энтропии после разделения выборкина две части"
      ],
      "metadata": {
        "id": "vg0KUzy3K1n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def information_gain(before_split: pd.Series, split_left: pd.Series, split_right: pd.Series) -> float:\n",
        "    e_before_split = entropy(before_split)\n",
        "    e_left, left_proportion = entropy(split_left), len(split_left) / len(before_split)\n",
        "    e_right, right_proportion = entropy(split_right), len(split_right) / len(before_split)\n",
        "    return (e_before_split - left_proportion * e_left - right_proportion * e_right)\n"
      ],
      "metadata": {
        "id": "_2dGoZ9YMocP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert abs(information_gain(pd.Series(['a', 'a', 'b', 'b']), pd.Series(['a', 'a']), pd.Series(['b', 'b'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'b', 'c', 'b']), pd.Series(['a', 'c']), pd.Series(['b', 'b'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'b', 'c', 'd']), pd.Series(['a', 'b']), pd.Series(['c', 'd'])) - 1.0) < eps\n",
        "assert abs(information_gain(pd.Series(['a', 'a', 'c', 'd']), pd.Series(['a', 'c']), pd.Series(['a', 'd'])) - 0.5) < eps\n"
      ],
      "metadata": {
        "id": "TwTMGgYWMwRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь напишем функцию, которая разбивает переданные данные на части по указанному количеству шагов, обучает модель на каждой части и возвращает среднюю метрику качества модели и название используемого классификатора"
      ],
      "metadata": {
        "id": "xL_U-ECsMwx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "\n",
        "def split_by_feature(x: pd.DataFrame, y: pd.Series, feature: str, steps: int = 100) -> Tuple[float, float]:\n",
        "    min_value, max_value = x[feature].min(), x[feature].max()\n",
        "    thresholds = np.linspace(min_value, max_value, steps)\n",
        "    best_gain, best_threshold = 0, None\n",
        "    x_parts = np.array_split(x[feature], steps)\n",
        "    y_parts = np.array_split(y, steps)\n",
        "    for threshold in thresholds:\n",
        "        ### Ваш код\n",
        "        info_gains = []\n",
        "        for i in range(steps):\n",
        "            y_true = y_parts[i]\n",
        "            split_left = y_true[x_parts[i] <= threshold]\n",
        "            split_right = y_true[x_parts[i] > threshold]\n",
        "            info_gains.append(information_gain(y_true, split_left, split_right))\n",
        "        mean_info_gain = sum(info_gains) / len(info_gains)\n",
        "        if mean_info_gain > best_gain:\n",
        "            bast_gain = mean_info_gain\n",
        "            best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_threshold\n"
      ],
      "metadata": {
        "id": "FZWo896qNAsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В описсанной ниже функции разбиения возвращается наилучший информационный выигрыш, соответствующий столбцу, индексу столбца и пороговому значению"
      ],
      "metadata": {
        "id": "R02DtMELNIlx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def split_by_features(x: pd.DataFrame, y: pd.Series, steps: int = 100) -> Tuple[float, str]:\n",
        "    best_gain, best_threshold, best_feature, best_idx = 0, None, None, None\n",
        "\n",
        "    for idx, feature in enumerate(x.columns):\n",
        "        ### Ваш код\n",
        "        thresholds = np.linspace(x[feature].min(), x[feature].max(), steps)\n",
        "        for threshold in thresholds:\n",
        "            split_left = y[x[feature] <= threshold]\n",
        "            split_right = y[x[feature] > threshold]\n",
        "            gain = information_gain(y, split_left, split_right)\n",
        "            if gain > best_gain:\n",
        "                best_gain = gain\n",
        "                best_feature = feature\n",
        "                best_idx = idx\n",
        "                best_threshold = threshold\n",
        "\n",
        "    return best_gain, best_feature, best_idx, best_threshold"
      ],
      "metadata": {
        "id": "np_jJZ25NKhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Опишем вершину принятия решений. В вершине мы либо храним правило, либо доминантный класс."
      ],
      "metadata": {
        "id": "7aRej8MCNh4d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PMn2TqmCMqR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Optional\n",
        "\n",
        "@dataclass\n",
        "class DecisionNode:\n",
        "    feature: Optional[str] = None\n",
        "    feature_idx: Optional[int] = None\n",
        "    threshold: Optional[float] = None\n",
        "    dominative_class: Any = None\n",
        "\n",
        "    @classmethod\n",
        "    def make_leaf(cls, values: pd.Series) -> 'DecisionNode':\n",
        "        class_counts = values.value_counts().to_dict()\n",
        "        dominative_class = max(class_counts, key=lambda c: class_counts[c])\n",
        "        return cls(dominative_class=dominative_class)\n",
        "\n",
        "    @classmethod\n",
        "    def make_node(\n",
        "        cls, best_feature: str, best_feature_idx: int, best_threshold: float\n",
        "    ) -> 'DecisionNode':\n",
        "        return cls(\n",
        "            feature=best_feature,\n",
        "            feature_idx=best_feature_idx,\n",
        "            threshold=best_threshold\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self) -> bool:\n",
        "        return bool(self.dominative_class)\n",
        "\n"
      ],
      "metadata": {
        "id": "DWHbaX1rNgVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим класс. Метод fit будет отвечать за тренировку классификатора. Он перебирает все признаки и все уникальные значения каждого признака, строит решающее правило на основе признака и порога, вычисляет ошибку классификации для каждого правила и выбирает правило с минимальной ошибкой.\n",
        "\n",
        "\n",
        "Метод predict использует сохраненный ранее узел дерева, который содержит информацию о признаке, пороге, левом и правом поддеревьях, исходных данных и метках листьев. Затем он рекурсивно проходит по дереву, начиная с корневого узла, и классифицирует новые данные, используя правила, сохраненные в каждом узле."
      ],
      "metadata": {
        "id": "56Lu9rjSSVP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionStump:\n",
        "    def __init__(self):\n",
        "        # Будем хранить внутри пня информацию о лучшем признаке и лучшей границе этого признака\n",
        "        self._node: Optional[DecisionNode] = None\n",
        "        self._left_node: Optional[DecisionNode] = None\n",
        "        self._right_node: Optional[DecisionNode] = None\n",
        "        self.feature_index = None\n",
        "    \n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series) -> None:\n",
        "        _, feature, feature_idx, threshold = split_by_features(X_train, y_train)\n",
        "        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n",
        "        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n",
        "        ### Ваш код\n",
        "        self.feature = None  # инициализируем признак, на котором делается разбиение\n",
        "        self.threshold = None  # инициализируем порог разбиения\n",
        "        self.alpha = None  # инициализируем вес классификатора\n",
        "        best_error = float(\"inf\")  # инициализируем минимальную ошибку классификации\n",
        "\n",
        "        for feature in X_train.columns:  # перебираем все признаки\n",
        "            for threshold in X_train[feature].unique():  # перебираем все уникальные значения признака\n",
        "                # строим правило на основе признака и порога\n",
        "                y_pred = np.where(X_train[feature] <= threshold, -1, 1)\n",
        "                # вычисляем ошибку классификации\n",
        "                error = np.sum(y_pred != y_train)\n",
        "                if error < best_error:  # если ошибка меньше текущей минимальной\n",
        "                    best_error = error  # обновляем минимальную ошибку\n",
        "                    self.feature = feature  # сохраняем признак и порог\n",
        "                    self.threshold = threshold\n",
        "\n",
        "        # вычисляем вес классификатора\n",
        "\n",
        "        self.alpha = 0.5 * np.log((1 - best_error) / best_error)\n",
        "        \n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
        "        # x = X.copy()\n",
        "        # node = self._node\n",
        "        # left_samples = x[x[node.feature] < node.threshold]\n",
        "        # right_samples = x[x[node.feature] >= node.threshold]\n",
        "        # ### Ваш код\n",
        "        # predictions = pd.Series(np.zeros(len(x)), index=x.index)\n",
        "\n",
        "        # while node:\n",
        "        #     if node.is_leaf:\n",
        "        #         predictions.loc[x.index.intersection(node.samples.index)] = node.label\n",
        "        #         break\n",
        "        #     left_mask = left_samples.index.isin(x.index)\n",
        "        #     right_mask = right_samples.index.isin(x.index)\n",
        "        #     predictions.loc[left_mask] = node.left.predict(left_samples)\n",
        "        #     predictions.loc[right_mask] = node.right.predict(right_samples)\n",
        "        #     node = node.left if x.loc[left_mask, node.feature].all() else node.right\n",
        "        \n",
        "        # return predictions\n",
        "\n",
        "        # создаем серию с предсказаниями и возвращаем ее\n",
        "        predictions = pd.Series(index=X.index)\n",
        "        for i, row in X.iterrows():\n",
        "            predictions[i] = self._predict_row(row)\n",
        "        return predictions\n",
        "\n",
        "    def _predict_row(self, row: pd.Series) -> int:\n",
        "        # делаем предсказание для одной строки\n",
        "        feature_value = row[self._node.feature_index]\n",
        "        if feature_value < self._node.threshold:\n",
        "            return self._left_node.prediction\n",
        "        else:\n",
        "            return self._right_node.prediction\n",
        "\n",
        "    def fit_predict(self, X: pd.DataFrame, y: pd.Series) -> pd.Series:\n",
        "        self.fit(X, y)\n",
        "        return self.predict(X)\n"
      ],
      "metadata": {
        "id": "KW8uEAdNSVlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим "
      ],
      "metadata": {
        "id": "_ZAHlC0NN5BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "reduced_df = df[df.target.apply(lambda t: t in ['setosa', 'versicolor'])]\n",
        "X, y = reduced_df.drop(columns=['target']), reduced_df.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.35, random_state=42)\n",
        "\n",
        "print(len(X_train), len(X_test))\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "stump = DecisionStump()\n",
        "stump.fit(X_train, y_train)\n",
        "y_pred = stump.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "stump._node.feature, stump._node.threshold\n",
        "\n",
        "@dataclass\n",
        "class DecisionNode:\n",
        "    # Решающие признаки остаются как есть\n",
        "    feature: Optional[str] = None\n",
        "    feature_idx: Optional[int] = None\n",
        "    threshold: Optional[float] = None\n",
        "    # Добавляются ссылки на левую и правую вершины\n",
        "    left: Optional['DecisionNode'] = None\n",
        "    right: Optional['DecisionNode'] = None\n",
        "    # Класс в листе остается как есть\n",
        "    dominative_class: Any = None\n",
        "\n",
        "    @property\n",
        "    def is_leaf(self) -> bool:\n",
        "        return bool(self.dominative_class)\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_impurity_spllit: float = 0.01, min_leaf_split=2, max_depth=None):\n",
        "        self._root: Optional[DecisionNode] = None\n",
        "        # В качестве простых эвристики для остановки возьмем:\n",
        "        # - максимальную глубину ветви\n",
        "        # - минимально допустимое число примеров в узле\n",
        "        # - долю \"примесей\" в узле\n",
        "        self._max_impurity_spllit = max_impurity_spllit\n",
        "        self._min_leaf_split = min_leaf_split\n",
        "        self._max_depth = max_depth\n",
        "\n",
        "    def _fit(self, X_train: pd.DataFrame, y_train: pd.Series, depth=0):\n",
        "        # проверим, можно ли вернуть лист\n",
        "        classes_probas = y_train.value_counts(normalize=True).to_dict()\n",
        "        dominative_class = max(classes_probas, key=lambda c: classes_probas[c])\n",
        "        impurity = 1 - classes_probas[dominative_class]\n",
        "        leaf_size = len(X_train)\n",
        "        if impurity <= self._max_impurity_spllit or leaf_size < self._min_leaf_split or (self._max_depth and depth >= self._max_depth):\n",
        "            return DecisionNode(dominative_class=dominative_class)\n",
        "        # если нет, продолжаем идти рекурсивно\n",
        "        # делаем перебор максимум в 10 точках\n",
        "        _, feature, feature_idx, threshold = split_by_features(X_train, y_train, 10)\n",
        "        if not feature:  # если не получается выбрать оптимальный признак для разбиения - завершаем перебор\n",
        "            return DecisionNode(dominative_class=dominative_class)\n",
        "        x_left, x_right = X_train[X_train[feature] < threshold], X_train[X_train[feature] >= threshold]\n",
        "        y_left, y_right = y_train[x_left.index], y_train[x_right.index]\n",
        "        left_node = self._fit(x_left, y_left, depth + 1)\n",
        "        right_node = self._fit(x_right, y_right, depth + 1)\n",
        "        return DecisionNode(\n",
        "            feature=feature,\n",
        "            feature_idx=feature_idx,\n",
        "            threshold=threshold,\n",
        "            left=left_node,\n",
        "            right=right_node,\n",
        "        )\n",
        "\n",
        "    def fit(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        self._root = self._fit(X_train, y_train)\n",
        "\n",
        "    def _predict(self, sample: pd.Series, node: DecisionNode) -> Any:\n",
        "        if node.is_leaf:\n",
        "            return node.dominative_class\n",
        "        if sample[node.feature] < node.threshold:\n",
        "            return self._predict(sample, node.left)\n",
        "        return self._predict(sample, node.right)\n",
        "\n",
        "    def predict(self, X: pd.DataFrame) -> pd.Series:\n",
        "        return X.apply(lambda sample: self._predict(sample, self._root), axis=1)\n",
        "\n",
        "    def fit_predict(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        self.fit(X_train, y_train)\n",
        "        return self.predict(X_train)\n",
        "X, y = df.drop(columns=['target']), df.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=42)\n",
        "\n",
        "print(len(X_train), len(X_test))\n",
        "tree = DecisionTree()\n",
        "tree.fit(X_train, y_train)\n",
        "y_pred = tree.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ-0FKdaZXIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вторая часть ДЗ"
      ],
      "metadata": {
        "id": "GKtOnum5ajUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импорт модулей"
      ],
      "metadata": {
        "id": "fo5AM45m8rPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('amazon_co-ecommerce_sample.csv').drop(columns=[\n",
        "    'product_name',\n",
        "    'index',\n",
        "    'uniq_id',\n",
        "    'customers_who_bought_this_item_also_bought',\n",
        "    'items_customers_buy_after_viewing_this_item',\n",
        "    'sellers',\n",
        "    'description', # text\n",
        "    'product_information', # text\n",
        "    'product_description', # text\n",
        "    'customer_questions_and_answers', # text\n",
        "    'customer_reviews', # text\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "fPAWhrX78stE",
        "outputId": "ff066cf5-4f2b-4465-f107-9dd675eea494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-61b7572c3f92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m df = pd.read_csv('amazon_co-ecommerce_sample.csv').drop(columns=[\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m'product_name'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'uniq_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m'customers_who_bought_this_item_also_bought'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1776\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1778\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1779\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1780\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 18 fields in line 318, saw 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Считывание таблицы и удаление некоторых колонок"
      ],
      "metadata": {
        "id": "7XaKMeiN8vQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getna(df):\n",
        "    print(\"Количество nan | имя топика | процент потери от общего числа\")\n",
        "    for column in df.columns:\n",
        "        #print(df[column])\n",
        "        print(df[column].isnull().sum(), \"|\", column, \"|\", df[column].isnull().sum() / len(df[column]) * 100, \"%\")\n",
        "\n",
        "getna(df)"
      ],
      "metadata": {
        "id": "6gIUCsGQ8x1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод кол-ва NaN-ов, имени топика и соотношения между общим кол-вом данных и NaN-ов"
      ],
      "metadata": {
        "id": "WpJu5-gk8zQf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выкидывать данные из датасета не стоит, потому что все эти фичи крайне вероятно, что\n",
        " напрямую или косвенно будут влиять на будущую цену товара\n",
        "\n",
        " Заполнение пропусков в числовых признаках:\n"
      ],
      "metadata": {
        "id": "u3uAPC0k9OIP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка со средним пользовательским рейтингом переводится в систему оценки от 0 до 1"
      ],
      "metadata": {
        "id": "HNkKosXY9kGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formean = sum([float(x.split()[0]) / float(x.split()[-2]) for x in df[\"average_review_rating\"] if type(x) == str])\n",
        "print(formean / len(df)) # Mean value\n",
        "df[\"average_review_rating\"] = df[\"average_review_rating\"].fillna(f\"{formean * 5} out of 5 stars\")\n",
        "df[\"average_review_rating\"] = [float(x.split()[0]) / float(x.split()[-2]) for x in df[\"average_review_rating\"]]\n",
        "print(df[\"average_review_rating\"])"
      ],
      "metadata": {
        "id": "4kfMRXDB9LE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с кол-вом ответов на вопросы заполняется, где недостача, средними значениями"
      ],
      "metadata": {
        "id": "QaCh5DRA9i7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_answered_questions\"] = df[\"number_of_answered_questions\"].astype(float)\n",
        "df[\"number_of_answered_questions\"] = df[\"number_of_answered_questions\"].fillna(df[\"number_of_answered_questions\"].mean())\n",
        "print(df[\"number_available_in_stock\"])\n"
      ],
      "metadata": {
        "id": "_SSsiM3A9hHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с оценками заполняется "
      ],
      "metadata": {
        "id": "HhhU8To395IU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].fillna(df[\"number_of_answered_questions\"]).astype(str)\n",
        "df[\"number_of_reviews\"] = [x.replace(\",\", \".\") for x in df[\"number_of_reviews\"]]\n",
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].astype(float)\n",
        "print(df[\"number_of_reviews\"])"
      ],
      "metadata": {
        "id": "EtL72d4p96Ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка кол-ва ревью преобразовывается в float тип и заполняется средними значениями"
      ],
      "metadata": {
        "id": "98occ7wcA-tr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].apply(lambda x: float(str(x).replace(\",\", '.')))\n",
        "df[\"number_of_reviews\"] = df[\"number_of_reviews\"].fillna(df[\"number_of_reviews\"].mean())\n",
        "print(df[\"number_of_reviews\"])"
      ],
      "metadata": {
        "id": "4UGNO5NOBJNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка цен преобразовывается к float и заполняется средними значениями"
      ],
      "metadata": {
        "id": "O8-r8z-CBLxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.price = df.price.apply(lambda x: float(str(x).replace(\"£\", \"\").replace(\",\", \"\").split()[0]))\n",
        "df.price = df.price.fillna(df.price.mean())\n",
        "print(df.price)"
      ],
      "metadata": {
        "id": "o6jHb4oxDO_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка количества товара в наличии заполняется нулями, если товар отсутствует"
      ],
      "metadata": {
        "id": "TrSz8q6pMyKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"number_available_in_stock\"] = df[\"number_available_in_stock\"].apply(lambda x: float(str(x).split()[0]))\n",
        "df[\"number_available_in_stock\"] = df[\"number_available_in_stock\"].fillna(0)\n",
        "print(df[\"number_available_in_stock\"])"
      ],
      "metadata": {
        "id": "xiHHkyqzM3n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Категориальные признаки"
      ],
      "metadata": {
        "id": "wzD8LxiKMDRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка производителей с помощью Ordinal encoding кодируется"
      ],
      "metadata": {
        "id": "FA_WhPWQDRTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[[\"manufacturer\"]]\n",
        "ll = OrdinalEncoder()\n",
        "X = ll.fit_transform(X)\n",
        "df[\"manufacturer\"] = X\n",
        "print(df[\"manufacturer\"])"
      ],
      "metadata": {
        "id": "KZFNF-grNGg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Колонка с категориями амазон - колонка, которая представляет собой еще один датафрейм. "
      ],
      "metadata": {
        "id": "iDXnCWbeNIAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ll = OneHotEncoder()\n",
        "df_category = df[\"amazon_category_and_sub_category\"].str.split(\">\", expand=True)\n",
        "df_category = ll.fit_transform(df_category.values)\n",
        "print(df_category)"
      ],
      "metadata": {
        "id": "YELwckL2NRLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yik4_vnX8mne"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    }
  ]
}